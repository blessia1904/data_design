{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "338b5faa",
   "metadata": {},
   "source": [
    "# List price:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "921795b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['$1,399\\r\\n          and 99 cents\\r\\n        ']\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent\n",
    "import requests\n",
    "\n",
    "ua = UserAgent()\n",
    "header = {'User-Agent':str(ua.chrome)} #requests to fake a browser visit\n",
    "url = \"https://www.tigerdirect.com/applications/SearchTools/item-details.asp?EdpNo=1501390\"\n",
    "keyword = \"p.list-price span.sr-only\"\n",
    "\n",
    "def get_content_from_website(url, keyword):\n",
    "\ttry:\n",
    "\t\tcontainer = []\n",
    "\t\tpage = requests.get(url, headers=header)\n",
    "        # Create a beautifulsoup object \n",
    "\t\tsoup = BeautifulSoup(page.text, 'lxml')\n",
    "\t\t# find class p that has \"list-price\" and class span that has \"sr-only\".\n",
    "\t\tlist_of_contents = soup.select(keyword)\n",
    "\t\tfor i in list_of_contents:\n",
    "\t\t\tcontainer.append(i.text)\n",
    "\t\n",
    "\texcept:\n",
    "\t\tprint(\"Problem with the connection...\")\n",
    "\treturn container\n",
    "\n",
    "container = get_content_from_website(url, keyword)\n",
    "print(container)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7baa10e9",
   "metadata": {},
   "source": [
    "## regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c35c87d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1,399'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "first_part = re.findall(r'(?:\\d+\\.)?\\d+,\\d+', container[0])\n",
    "first_part[0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff1ee53c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'99'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_part = re.findall(r'(?:\\b\\d{2})\\b', container[0])\n",
    "second_part[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd9f528d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1399.99\n"
     ]
    }
   ],
   "source": [
    "first_part[0] = first_part[0].replace(\",\",\"\")\n",
    "current_price = first_part[0] + \".\" + second_part[0]\n",
    "\n",
    "print(current_price)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d9db84",
   "metadata": {},
   "source": [
    "# Current price:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64ad9104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['$1,029\\r\\n          and 99 cents\\r\\n        ']\n"
     ]
    }
   ],
   "source": [
    "url=\"https://www.tigerdirect.com/applications/SearchTools/item-details.asp?EdpNo=1501390\"\n",
    "keyword=\"p.final-price span.sr-only\"\n",
    "\n",
    "container_list = get_content_from_website(url,keyword)\n",
    "print(container_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b3e400",
   "metadata": {},
   "source": [
    "## regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c040d98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1,029'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "first_part_list = re.findall(r'(?:\\d+\\.)?\\d+,\\d+', container_list[0])\n",
    "first_part_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc899484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'99'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_part_list = re.findall(r'(?:\\b\\d{2})\\b', container_list[0])\n",
    "second_part_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81243574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1029.99\n"
     ]
    }
   ],
   "source": [
    "first_part_list[0] = first_part_list[0].replace(\",\",\"\")\n",
    "list_price = first_part_list[0] + \".\" + second_part_list[0]\n",
    "\n",
    "print(list_price)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0e7434",
   "metadata": {},
   "source": [
    "# US News"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5ae395",
   "metadata": {},
   "source": [
    "## Top stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bf72656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['U.S. Hits Debt Limit as Parties Squabble', 'Biden: ‘No Regrets’ Over Documents']\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent\n",
    "import requests\n",
    "\n",
    "ua = UserAgent()\n",
    "url=\"https://www.usnews.com/\"\n",
    "header = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.90 Safari/537.36\"}\n",
    "\n",
    "try:\n",
    "    container = []\n",
    "    page = requests.get(url, headers=header, timeout=5)\n",
    "    # Create a beautifulsoup object \n",
    "    soup = BeautifulSoup(page.text, 'lxml')\n",
    "    # Find the paragraph that have \"top stories\" in it\n",
    "    list_of_contents = soup.find_all(\"p\", string=\"Top Stories\")\n",
    "    for tag in list_of_contents:\n",
    "        nextone = tag.next_sibling # find the part next to \"top stories,\" which is the division that includes the 2 sub top stories \n",
    "        top_stroies = nextone.select(\"h3.story-headline\") # then find the 2 sub top stories that are in class h3 with \"story-headline\"\n",
    "    for i in top_stroies:\n",
    "        container.append(i.text)\n",
    "except:\n",
    "    print(\"Problem with the connection...\")\n",
    "print(container)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b4e14f",
   "metadata": {},
   "source": [
    "## Second top story's URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "699b3d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "https://www.usnews.com/news/politics/articles/2023-01-19/biden-says-he-has-no-regrets-about-his-handling-of-documents\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent\n",
    "import requests\n",
    "\n",
    "ua = UserAgent()\n",
    "url=\"https://www.usnews.com/\"\n",
    "header = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.90 Safari/537.36\"}\n",
    "\n",
    "second_story_title = ''.join(container[1])\n",
    "\n",
    "try:\n",
    "    links = []\n",
    "    page = requests.get(url, headers=header, timeout=5)\n",
    "    # Create a beautifulsoup object \n",
    "    soup = BeautifulSoup(page.text, 'lxml')\n",
    "    list_of_contents = soup.find_all(\"h3\", string=second_story_title) # find the second top story\n",
    "    for tag in list_of_contents: # since list_of_contents is a list, need to get a class to use the function find.all\n",
    "        for i in tag.find_all('a'):\n",
    "            links.append(i.get('href')) # get the url link for the second top story\n",
    "except:\n",
    "    print(\"Problem with the connection...\")\n",
    "\n",
    "links = ''.join(links)\n",
    "print(type(links))\n",
    "print(links)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54657c8f",
   "metadata": {},
   "source": [
    "## Second story's header + 3 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b4f8650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biden: ‘No Regrets’ Over Documents\n",
      "President Biden told reporters Thursday that he has no regrets about how the administration handled the disclosure of the discovery of classified documents at his home and off-White House campus office, saying he was confident the investigations would show nothing untoward had occurred.\"As soon as we found the handful of documents were filed in the wrong place, we immediately turned them over to the Archives and the Justice Department,\" Biden said from a pier at Seacliff State Beach in Aptos, California, where he was delivering remarks after touring storm damage in the state.Chiding reporters briefly for asking about the documents instead of the emergency response to the California storm, Biden added, \"We're fully cooperating and looking forward to this being resolved quickly.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent\n",
    "import requests\n",
    "\n",
    "ua = UserAgent()\n",
    "url= links\n",
    "header = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.90 Safari/537.36\"}\n",
    "\n",
    "try:\n",
    "    title = []\n",
    "    body = []\n",
    "    page = requests.get(url, headers=header, timeout=5)\n",
    "    # Create a beautifulsoup object \n",
    "    soup = BeautifulSoup(page.text, 'lxml')\n",
    "    list_of_contents = soup.select(\"h1.Heading-sc-1w5xk2o-0\") # Find the header with h1 class\n",
    "    for i in list_of_contents:\n",
    "        title.append(i.text) # store the header to \"title\"\n",
    "        \n",
    "    body_contents = soup.find_all(\"div\", class_=\"Raw-slyvem-0 bCYKCn\") # Find the article's contents\n",
    "    for tag in body_contents: # get all paragraphs\n",
    "        for i in tag.find_all('p'):\n",
    "            body.append(i.text)\n",
    "            \n",
    "    holder = []\n",
    "    final_result = []\n",
    "    for a in body: # use \". \" to split all sentences\n",
    "        split_body = a.split(\". \")\n",
    "        holder+=split_body # store separate sentences to \"holder\"\n",
    "    counter = 0\n",
    "    for sentence in holder:\n",
    "        if sentence != \"\":\n",
    "            if sentence.strip()[-1] != \".\": # add back period to sentences\n",
    "                final_result.append(sentence+\".\")\n",
    "            else:\n",
    "                final_result.append(sentence)\n",
    "            counter += 1\n",
    "        if counter >= 3: # only select the first 3 sentences\n",
    "            break\n",
    "    \n",
    "except:\n",
    "    print(\"Problem with the connection...\")\n",
    "    \n",
    "\n",
    "title = ''.join(title)\n",
    "print(title)\n",
    "\n",
    "final_result = ''.join(final_result)\n",
    "print(final_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15adbcbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2799fd5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MongoDB",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16 (main, Jan 11 2023, 10:02:19) \n[Clang 14.0.6 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "952df2330b1104b81be3d19cbd072d60e955b5200361a0dedcb40e20354b0cfc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
